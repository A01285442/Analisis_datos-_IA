import streamlit as st
import pandas as pd
from google import genai
from google.genai import types
from dotenv import load_dotenv
import os

# --- Configuraci칩n Inicial ---
st.set_page_config(
    page_title="Gemini Chat Dimex", 
    layout="wide"
)

# 1. Cargar variables de entorno (asumiendo que tu archivo .env tiene GEMINI_API_KEY o GOOGLE_API_KEY)
load_dotenv()

# Determinar qu칠 variable de entorno usar
api_key = os.getenv("GEMINI_API_KEY")

# 2. Inicializar cliente Gemini
try:
    if not api_key:
        st.error("Error: La clave API (GEMINI_API_KEY) no fue encontrada en las variables de entorno.")
        st.stop()
        
    # Inicializaci칩n expl칤cita con la clave obtenida
    client = genai.Client(api_key=api_key) 
    
except Exception as e:
    st.error(f"Error al inicializar el cliente Gemini: {e}")
    st.stop()

# --- Definici칩n de Roles (Instrucciones del Sistema) ---

ROLES = {
    "Riesgo": "Eres un analista de riesgos experto. Tu tarea es resumir datos de portafolios y destacar cualquier anomal칤a o alerta de cambio significativa. Responde de forma concisa y profesional.",
    "Cobranza": "Eres un asesor de cobranza. Tu objetivo es sugerir acciones de cobro y priorizar cuentas bas치ndote en la informaci칩n proporcionada. Usa un tono motivacional y directo.",
    "Servicio": "Eres un especialista de servicio al cliente. Responde a consultas frecuentes sobre productos Dimex de manera clara, amable y precisa. Si no conoces la respuesta, indica que la buscar치s.",
    "Fraude": "Eres un experto en prevenci칩n de fraude. Identifica patrones sospechosos en los datos y valida la informaci칩n din치micamente. Pide m치s detalles si es necesario para la validaci칩n.",
}

# --- Funciones de L칩gica de la Aplicaci칩n ---

# def load_and_prepare_knowledge(uploaded_file):
#     """Carga un archivo Excel y prepara el contenido como string de contexto."""
#     if uploaded_file is not None:
#         try:
#             # Leer la primera hoja del Excel
#             df = pd.read_excel(uploaded_file)
            
#             # Convertir las primeras 10 filas del DataFrame a un formato de texto (CSV o Markdown)
#             # Esto SIMULA el proceso de RAG, donde solo inyectamos datos relevantes.
#             context_string = f"DATOS DE CONOCIMIENTO:\n\n{df.head(10).to_markdown(index=False)}"
#             st.success("Datos cargados exitosamente. El modelo usar치 las primeras 10 filas como contexto.")
#             return context_string
#         except Exception as e:
#             st.error(f"Error al leer el archivo Excel: {e}")
#             return ""
#     return ""

# def load_and_prepare_knowledge(uploaded_file):
#     """Carga un archivo Excel y limita el contenido para evitar sobrecarga."""
#     if uploaded_file is not None:
#         try:
#             df = pd.read_excel(uploaded_file)
            
#             # **NUEVO:** Limitar a, por ejemplo, 5 filas y las primeras 5 columnas
#             df_limited = df.head(5).iloc[:, :5] 
            
#             context_string = f"DATOS DE CONOCIMIENTO:\n\n{df_limited.to_markdown(index=False)}"
            
#             # **NUEVO:** A침adir una verificaci칩n de longitud simple (por ejemplo, 1000 caracteres)
#             if len(context_string) > 1000:
#                 st.warning("El contexto generado es muy largo. Solo se usar치n los primeros 1000 caracteres.")
#                 context_string = context_string[:1000]

#             st.success("Datos cargados exitosamente y limitados para el contexto.")
#             return context_string
#         except Exception as e:
#             st.error(f"Error al leer el archivo Excel: {e}")
#             return ""
#     return ""

def load_and_prepare_knowledge(uploaded_file):
    if uploaded_file is not None:
        try:
            df = pd.read_excel(uploaded_file)
            
            # *** CAMBIO CRUCIAL: Solo las 2 primeras filas y 3 primeras columnas ***
            df_limited = df.head(2).iloc[:, :3] 
            
            context_string = f"DATOS DE CONOCIMIENTO (LIMITADOS):\n\n{df_limited.to_markdown(index=False)}"
            
            st.success("Datos cargados exitosamente. Contexto MUY LIMITADO para prueba.")
            return context_string
        except Exception as e:
            st.error(f"Error al leer el archivo Excel: {e}")
            return ""
    return ""
def generate_response(role_key, user_prompt, context_data):
    """Genera una respuesta usando el modelo de Gemini, inyectando el rol y el contexto."""
    
    # 1. Definir la instrucci칩n del sistema (el comportamiento de la 'Gem')
    system_instruction = ROLES.get(role_key, ROLES["Servicio"])
    
    # 2. Construir el prompt final inyectando el contexto
    full_prompt = f"{context_data}\n\n[INSTRUCCI칍N ESPEC칈FICA DE LA TAREA: {role_key}]\n\nPregunta del Usuario: {user_prompt}"

    try:
        # Llamada a la API de Gemini
        response = client.models.generate_content(
            model='gemini-2.5-flash', # Un modelo r치pido y eficiente
            contents=full_prompt,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction
            )
        )
        return response.text
    except Exception as e:
        return f"ERROR AL LLAMAR A LA API: {e}"

# --- Interfaz de Streamlit ---

st.title("游뱄 Asistente Dimex Personalizado (Gem-Simulado)")
st.subheader("Selecciona un 치rea para personalizar el comportamiento del chat.")

# 츼rea de selecci칩n de Rol y subida de Archivo (Sidebar)
with st.sidebar:
    st.header("丘뙖잺 Configuraci칩n del Asistente")
    
    # Selector de Rol
    selected_role = st.selectbox(
        "Selecciona el 츼rea (Comportamiento del Chat):",
        list(ROLES.keys()),
        key="role_selector"
    )
    st.info(f"Comportamiento Actual: **{selected_role}**")
    
    st.header("游닋 Cargar Conocimiento (Excel)")
    uploaded_file = st.file_uploader(
        "Sube un archivo Excel (.xlsx) para inyectar datos de conocimiento:", 
        type=["xlsx"]
    )
    
    # Cargar y preparar el contexto al cambiar el archivo
    knowledge_context = load_and_prepare_knowledge(uploaded_file)
    st.session_state["knowledge_context"] = knowledge_context


# Inicializar el historial de chat si no existe
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Hola! Selecciona un 치rea y sube un archivo (opcional) para empezar."}]

# Mostrar mensajes anteriores
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar la entrada del usuario
if prompt := st.chat_input("Escribe tu consulta..."):
    # A침adir el mensaje del usuario al historial
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar la respuesta
    with st.spinner(f"El asistente de {selected_role} est치 pensando..."):
        # La funci칩n clave llama a generate_response
        response_text = generate_response(selected_role, prompt, st.session_state["knowledge_context"])
    
    # Mostrar la respuesta del asistente
    with st.chat_message("assistant"):
        st.markdown(response_text)
    
    # A침adir la respuesta del asistente al historial
    st.session_state["messages"].append({"role": "assistant", "content": response_text})